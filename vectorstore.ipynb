{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Renhwai\\Documents\\GitHub\\genai_job_candidate_agent\\agentenv2\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "import textwrap\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import streamlit as st\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "docdir='rag_docs/'\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text_file=open(docdir+'Job_summary.txt')\n",
    "#text_file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_summary=\"\"\"\n",
    "**Job Position Name:** Business Analyst - SAP Billing (Cluster Office)\n",
    "\n",
    "**Company Name:** Synapxe\n",
    "\n",
    "**Job Responsibilities:**\n",
    "\n",
    "* Support strategic planning, integration, data analytics, and technology innovation for hospital operations\n",
    "* Manage projects related to redevelopment, renovation, and relocation\n",
    "* Support funding requests and review forums\n",
    "* Manage stakeholders and collaborate with project teams\n",
    "* Monitor project implementation status and provide updates\n",
    "* Provide operational support and handle ad hoc tasks\n",
    "\n",
    "**Job Requirements:**\n",
    "\n",
    "* Degree in Computer Science, Computer Engineering, Information Technology, or related field\n",
    "* 7-10 years of relevant work experience\n",
    "* Experience with SAP billing systems or tertiary care hospital billing systems\n",
    "* Experience managing Finance/Billing Systems\n",
    "* CITPM / PMP certification (advantageous)\n",
    "* Excellent problem-solving, communication, and interpersonal skills\n",
    "* Strong organizational and customer service skills\n",
    "* Experience in healthcare industry (advantageous)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explain descriptive, predictive, and prescriptive analytics\n",
      "how can you handle missing values in a dataset\n",
      "Job descriptions of opening position shared by Hiring Manager\n",
      "job_summary\n",
      "please share about your data analysis projects from your past working experience\n",
      "please share about your development projects\n",
      "what are some common data visualization tools you have used\n",
      "what are the best methods for data cleaning\n",
      "what are your strengths and weaknesses as a data analyst\n",
      "what is Overfitting\n",
      "what is Time Series analysis\n",
      "which are the technical tools that you have used for analysis and presentation purposes\n",
      "why should we hire you\n"
     ]
    }
   ],
   "source": [
    "for i in os.listdir(docdir):\n",
    "    print(i.replace('.txt',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_knowledge():\n",
    "    table=pd.DataFrame(columns=['document', 'content','embedding','relevant score'])\n",
    "    i=0\n",
    "    for doc in os.listdir(docdir):\n",
    "        docsplit=TextLoader(docdir+doc,encoding='utf8').load_and_split(text_splitter)\n",
    "        for chunk in docsplit:\n",
    "            embedding = genai.embed_content(model='models/text-embedding-004',content=chunk.page_content,task_type=\"retrieval_query\")\n",
    "            table.loc[i]=[chunk.metadata['source'],chunk.page_content,embedding['embedding'],0]\n",
    "            i=i+1\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "table=update_knowledge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_knowledge(query, table):\n",
    "  query_embedding = genai.embed_content(model='models/text-embedding-004',\n",
    "                                        content=query,\n",
    "                                        task_type=\"retrieval_query\")\n",
    "  table['relevant score'] = np.dot(np.stack(table['embedding']), query_embedding[\"embedding\"])\n",
    "  return table['content'].iloc[np.argmax(table['relevant score'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(prompt, job_summary, passage):\n",
    "  escaped = passage.replace(\"'\", \"\").replace('\"', \"\").replace(\"\\n\", \" \")\n",
    "  prompt = textwrap.dedent(\"\"\"\n",
    "  You are a helpful and informative bot that answers questions using text from the reference passage included below.\n",
    "  Your are a digital twin of a job applicant, Kong Ren Hwai. KNOWLEDGE below are how Kong Ren Hwai answer interview question.\n",
    "  You will answer job interviewer's PROMPT using Kong Ren Hwai's perspective. Such as:\n",
    "  Question: What is your name?\n",
    "  Answer: My name is Kong Ren Hwai, I am looking for job role in Business Analyst, Data Analyst or Investment Analyst.\n",
    "  The JOB DESCRIPTION is given below, and you will reply PROMPT below with YOUR KNOWLEDGE below;\n",
    "  IF the JOB DESCRIPTION and YOUR KNOWLEDGE are not related to PROMPT, you can ignore JOB DESCRIPTION and YOUR KNOWLEDGE during answering.\n",
    "  PROMPT: {prompt}\n",
    "  JOB DESCRIPTION: {job_summary}\n",
    "  KNOWLEDGE: {passage}\n",
    "  \"\"\").format(prompt=prompt, job_summary=job_summary,passage=escaped)\n",
    "\n",
    "  return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel('gemini-1.0-pro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on my experience in data engineering, I believe the best approach is to employ a multi-step data cleaning process specifically tailored for production data. This includes data understanding, data integrity checks, simple exploratory data analysis (EDA), and outlier investigation and treatment. By following this process, I have been able to effectively cleanse and prepare data for various projects, ensuring its accuracy and reliability for downstream analysis and decision-making.'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt='What is the best way to perform data engineering?'\n",
    "passage=retrieve_knowledge(prompt, table)\n",
    "answer = model.generate_content(make_prompt(prompt, job_summary, passage))\n",
    "answer.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list=[]\n",
    "relevant_knowledge=table.loc[(table['relevant score']>0.5)].sort_values('relevant score',ascending=False).head(3)['content']\n",
    "i=1\n",
    "for t in relevant_knowledge.apply(lambda x: x.replace(\"\\ufeff\", \"\")):\n",
    "    text_list.append(\"KNOWLEDGE \"+str(i)+\": \"+t)\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['KNOWLEDGE 1: I employed a multi-step data cleaning process specifically tailored for production data. First step is about data understanding. Prior to cleaning, I familiarized myself with the data acquisition process. This involved determining the type of data collected (raw sensor data, aggregated data), any pre-processing applied on equipment (sensor offset, aggregation), and what are the the meanings and purposes of each sensor parameter. Next step I will perfrom data integrity checks. This stage focused on identifying and rectifying inconsistencies involve checking for missing values (nulls) and duplicates, cross-referencing with the manufacturing tracking system to ensure all processed units (e.g., wafers) have corresponding data entries. After that, I will do simple exploratory Data Analysis (EDA). Simple visualizations like data distribution plots and scatter plots (Y vs. X) were created to gain insights into data distribution patterns and identify potential outliers. Finally, those outliers were investigated to determine the cause and decide on appropriate treatment methods. This might involve data transformation techniques like normalization if necessary.',\n",
       " \"KNOWLEDGE 2: One of my strengths is, I excel at consolidating existing resources and thinking outside the box. I believe data analysis goes beyond just analyzing data; it's about presenting insights and transforming them into actionable plans. I actively seek out additional data sources and techniques beyond initial project requirements. Other than that, I emphasize on communication and collaboration, ensuring stakeholders receive a comprehensive view. I readily collaborate with other teams to identify user needs and explore possibilities like building dashboards or automating data access. Lastly, I think I have an edge over other data specialist in domain expertise. My diverse background in engineering, manufacturing, and finance allows me to assist other data specialists with troubleshooting and hypothesis generation. For my weakness, I believe I have to focs on developing programming skills. I am actively working to improve my programming skills, particularly in Python. While I have a basic understanding of Python, Google Cloud Platform, BigQuery, Snowflake, Jira, Streamlit, and Tableau, I sometimes rely on online resources to complete complex coding tasks. This can lead to longer completion times. However, I am a quick learner and highly motivated to overcome this limitation through dedicated learning and on-the-job training.\",\n",
       " \"KNOWLEDGE 3: In my data analysis experience, I've utilized a variety of tools to create impactful visualizations. For technical presentations I heavily relied on JMP and Excel to present technical and engineering data effectively; When I was Business Analyst, I transitioned to Tableau and Spotfire to create interactive dashboards that communicated insights to stakeholders and users; Throughout my professional development during career break, I've learned Python and its visualization libraries like Matplotlib, Plotly, and Streamlit. This allows me to automate and create more customized and interactive visualizations when needed.\"]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"KNOWLEDGE 1: There are few stages to approach the missing value problem. The first stage is missing data assessment. I start by calculating missing value percentages for both columns and rows, and use visualizations like heatmaps or bar charts to understand the distribution and amount of missingness. Then, I will investigate systematic missingness. For example, a missing sale data for an entire outlet, this might indicate data acquisition errors requiring potential fixes. In the second stage, I will conduct missing data treatment. My decision on handling missing values depends on the extent and impact on the data. For columns or rows with a high missing ratio (like, more than 40%).Then if data size and diversity allow, I might further remove data with a missing value percentage exceeding a threshold (example, more than 10%).The last resort is to transform the missing value. I will consider techniques like mean/median imputation for numerical data or encoding for categorical data. I will only use this if the data size or distribution are limited. In the final stage, after any missing data treatment, I assess the final dataset for potential bias arising from missing values. This might involve checking data distribution or missing value patterns, and record down the treatment process into data cleaning record.KNOWLEDGE 2: In my data analysis experience, I've utilized a variety of tools to create impactful visualizations. For technical presentations I heavily relied on JMP and Excel to present technical and engineering data effectively; When I was Business Analyst, I transitioned to Tableau and Spotfire to create interactive dashboards that communicated insights to stakeholders and users; Throughout my professional development during career break, I've learned Python and its visualization libraries like Matplotlib, Plotly, and Streamlit. This allows me to automate and create more customized and interactive visualizations when needed.KNOWLEDGE 3: I employed a multi-step data cleaning process specifically tailored for production data. First step is about data understanding. Prior to cleaning, I familiarized myself with the data acquisition process. This involved determining the type of data collected (raw sensor data, aggregated data), any pre-processing applied on equipment (sensor offset, aggregation), and what are the the meanings and purposes of each sensor parameter. Next step I will perfrom data integrity checks. This stage focused on identifying and rectifying inconsistencies involve checking for missing values (nulls) and duplicates, cross-referencing with the manufacturing tracking system to ensure all processed units (e.g., wafers) have corresponding data entries. After that, I will do simple exploratory Data Analysis (EDA). Simple visualizations like data distribution plots and scatter plots (Y vs. X) were created to gain insights into data distribution patterns and identify potential outliers. Finally, those outliers were investigated to determine the cause and decide on appropriate treatment methods. This might involve data transformation techniques like normalization if necessary.KNOWLEDGE 4: One of my strengths is, I excel at consolidating existing resources and thinking outside the box. I believe data analysis goes beyond just analyzing data; it's about presenting insights and transforming them into actionable plans. I actively seek out additional data sources and techniques beyond initial project requirements. Other than that, I emphasize on communication and collaboration, ensuring stakeholders receive a comprehensive view. I readily collaborate with other teams to identify user needs and explore possibilities like building dashboards or automating data access. Lastly, I think I have an edge over other data specialist in domain expertise. My diverse background in engineering, manufacturing, and finance allows me to assist other data specialists with troubleshooting and hypothesis generation. For my weakness, I believe I have to focs on developing programming skills. I am actively working to improve my programming skills, particularly in Python. While I have a basic understanding of Python, Google Cloud Platform, BigQuery, Snowflake, Jira, Streamlit, and Tableau, I sometimes rely on online resources to complete complex coding tasks. This can lead to longer completion times. However, I am a quick learner and highly motivated to overcome this limitation through dedicated learning and on-the-job training.KNOWLEDGE 5: In my role as Senior Industry 4.0 Analyst, I will help the team to identify appropriate data sources, mostly are in Google Cloud Platform and Snowflake. Utilizing my proficiency in BigQuery and Snowflake, I performed basic data queries, including joining tables and manipulating data columns. For data exploration, analysis, and visualization, I leveraged tools like Tableau, Spotfire, JMP, and Excel to provide data insight and hypothesis to the team. While these are excellent tools for analyzing small and static datasets, they can become cumbersome for larger datasets with numerous variables (50-100+). To address this challenge, I pursued professional development in Python during my career break. I practiced using Python libraries on public datasets and participated in Kaggle and Hugging Face competitions. I leverage Python to automate tasks such as high missing value column removal, variable looping for t-tests, and summarizing potentially important variables. Furthermore, I can perform basic model training, feature engineering, and hyperparameter autotuning using Python. Finally, I used PowerPoint for presentation. I worked directly under director, we had regulary reviewes with executives from internal and external stakeholder, up VP. I have been trained to how to tell a good story in review through designing the slide and flow of presentation, and interact and influencing the audiences.KNOWLEDGE 6: I believe I possess the ideal combination of technical skills and soft skills to excel in this position. For technical data skills, I have experience using various tools for data acquisition, cleaning, analysis, and visualization (e.g., BigQuery, Snowflake, Tableau, Python libraries). I have gained a strong analytical foundation from working as a Semiconductor Technology Development Engineer, which dealt with problems solving and technical risk assessment. For soft skills, I am good at effective collaboration. During my tenure as Senior Industrial 4.0 Analyst, I successfully managed smart manufacturing projects and collaborated effectively with cross-functional teams (e.g., data engineers, data scientists, business units) across multiple locations (eg: US, India, China), in order to manage project progress and priority. Most importantly,I have experience in stakeholder management. I have been working directly under the director and prepare weekly/monthly presentations and newsletter updates to executives up to vice president level. I am passionate about learning. I am constantly seeking new knowledge and skills, as evidenced by my completion of CFA (Chartered Financial Analyst) Level 1 and pursuit of Level 2. I have a diverse background ranging from data analysis, engineering and finance which could bring a unique perspective to data analysis. By combining my technical skills, strong communication abilities, and collaborative spirit, I am confident I can make a significant contribution to your team.\""
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\".join(text_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
